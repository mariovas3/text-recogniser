# lightning.pytorch==2.2.1
seed_everything: 0
trainer:
  accelerator: auto
  strategy: auto
  devices: auto
  num_nodes: 1
  precision: null
  # get wandb logger;
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: "IAMOgAndSynthParagraphs"
      log_model: "all"
      name: "local_run"
  # get callbacks;
  callbacks:
    - class_path: lightning.pytorch.callbacks.DeviceStatsMonitor
    # <start> hacky way to get wandb to watch grads in a thread safe way;
    # look at run_experiment.py/MyTrainer class;
    # - class_path: model_package.lit_models.callbacks.SetLoggerWatch
    # <end> hacky way to get wandb to watch grads in a thread safe way;
    - class_path: model_package.lit_models.callbacks.LogPredsCallback
    - class_path: model_package.lit_models.callbacks.LogTrainPredsCallback
  fast_dev_run: false
  max_epochs: 4
  min_epochs: null
  max_steps: -1
  min_steps: null
  max_time: null
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
  limit_predict_batches: null
  overfit_batches: 0.0
  val_check_interval: null
  check_val_every_n_epoch: 1
  num_sanity_val_steps: null
  log_every_n_steps: null
  enable_checkpointing: null
  enable_progress_bar: null
  enable_model_summary: null
  accumulate_grad_batches: 1
  gradient_clip_val: null
  gradient_clip_algorithm: null
  # make deterministic;
  deterministic: true
  benchmark: null
  inference_mode: true
  use_distributed_sampler: true
  profiler: null
  detect_anomaly: false
  barebones: false
  plugins: null
  sync_batchnorm: false
  reload_dataloaders_every_n_epochs: 0
  default_root_dir: null
model:
  resnet_config:
    in_channels: [1, 4, 4, 8, 8, 16, 16, 32, 32]
    out_channels: [4, 4, 8, 8, 16, 16, 32, 32, 64]
    kernel_sizes: [5, 3, 3, 3, 3, 3, 3, 3, 3]
    strides: [2, 1, 2, 1, 2, 1, 2, 1, 2]
  tf_dim: 64
  tf_fc_dim: 128
  tf_nhead: 4
  tf_dropout: 0
  tf_num_layers: 4
  lr: 0.001
  with_enc_pos: false
my_model_checkpoint:
  dirpath: null
  filename: null
  monitor: validation/loss
  verbose: false
  save_last: true
  save_top_k: 1
  save_weights_only: false
  mode: min
  auto_insert_metric_name: true
  every_n_train_steps: null
  train_time_interval: null
  # if both every_n_epochs and trainer.check_val_every_n_epoch
  # are given, than checkpoint created after each epoch that is
  # perfectly divided by my_model_checkpoint.every_n_epochs
  # and trainer.check_val_every_n_epoch
  every_n_epochs: 50
  save_on_train_epoch_end: null
  enable_version_counter: true
ckpt_path: null
data:
  class_path: model_package.data.IAMOgAndSynthParagraphs
  init_args:
    augment: false
    batch_size: 128
    num_workers: 8
    gpus: 0
